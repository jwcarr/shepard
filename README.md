shepard
=======

This repository contains data and code from Carr, Smith, Culbertson, and Kirby (submitted). It includes Python code for running a Bayesian iterated learning model of the emergence of semantic categories (plus the raw data that we generated from this model), and code for running online experiments that test the predictions of the model (plus the participant data we collected). Various other Python and R scripts are also included for replicating the analysis. All Python code in this repo was written for Python 3; little attempt has been made to ensure compatibility with Python 2. Much of the Python code requires Numpy, Scipy, and Matplotlib.

The top-level structure of the repo is:

- ```code/```: All Python code used for the model and analysis

- ```data/```: All the raw data files

- ```experiments/```: Node.js code for the experiments

- ```illustrations/```: Illustrations created in Affinity Designer

- ```manuscript/```: LaTeX source and EPS figures for the manuscript

- ```stats/```: R scripts for reproducing the statistics

- ```supplementary/```: Various supplementary items

- ```visuals/```: Various visualizations

Data
----

All experimental data can be found in the ```data/experiments/``` directory. This includes the following files:

- ```exp1_participants.json```: Raw JSON data for all participants in Experiment 1 (one participant per line). Personal information has been removed from the original file, such as participant IP addresses and user IDs.

- ```exp1_stats.csv```: CSV data for Experiment 1 (used by the R script to compute the stats). This file may be regenerated using the ```generate_csv_for_stats()``` function in ```code/exp1_results.py```.

- ```exp2_chains.json```: JSON data for all chains in Experiment 2. This is flattened version of the original file (giving it a similar structure to the model result JSON files) and removes personal participant information.

- ```exp2_participants.json```: Raw JSON data for all participants in Experiment 2 (one participant per line). Personal information has been removed from the original file, such as participant IP addresses and user IDs.

- ```exp2_stats.csv```: CSV data for Experiment 2 (used by the R script to compute the stats). This file may be regenerated using the ```generate_csv_for_stats()``` function in ```code/exp2_results.py```.

The model and model-fit data has been compressed into zip files. Uncompressed, this data consumes about 1GB of disk space. If you wish to access the raw model or model-fit data, you will need to uncompress the following files:

- ```data/model_inf.zip```: 97 JSON files, each for a different run of the model under the informativeness prior. Each JSON file contains 100 chains of 50 generations. Files are named following the pattern ```weight_noise_bottleneck_exposures.json```.

- ```data/model_sim.zip```: 49 JSON files, each for a different run of the model under the simplicity prior. Each JSON file contains 100 chains of 50 generations. Files are named following the pattern ```weight_noise_bottleneck_exposures.json```.

- ```data/modelfit.zip```: Various raw data files and pickled scikit-optimize ```result``` objects. Includes the data_in–data_out pairs for 168 participants (generated by ```code/mf_extract_data_in_out.py```) and log likelihood results under random and candidate parameter settings (generated by ```code/mf_sample.py```).

The ```data``` directory also contains:

- ```8x8_solutions.json```: Rectangular decomposition solutions for Experiment 2 data.

- ```rectlang.zip```: The raw data (generated by ```exp2_chunkify.py```) used to compute ```8x8_solutions.json``` (this raw data should not be of much interest)

The structure of the JSON and CSV files should be self-explanatory. The JSON files may be loaded using functions provided in ```tools.py```, for example:

```
>>> import tools
>>> model_dataset = tools.read_json_file('../data/model_sim/1.0_0.01_2_2.json')
>>> exp2_dataset = tools.read_json_file('../data/experiments/exp2_chains.json')
```

These datasets are Python dictionaries that can be inspected, manipulated, and iterated over like this:

```
>>> model_dataset['chains'][16]['generations'][32]['lang_complexity'] # Complexity of chain 16, generation 32
49.44161477482669
>>> exp2_dataset['chains'][4]['generations'][8]['prod_complexity'] # Complexity of chain 4, generation 8
465.36882864375775
```

Model
-----

The code to run the model is in ```code/model.py```. This Python script is well documented and should allow the reader to run a simple iterated learning chain. The script is designed to be run from the command line. For example, the following command will run a single chain of 50 generations under the simplicity prior and the same basic parameter settings reported in the paper.

```
python model.py my/results/directory/ 1 --generations 50 --height 8 --width 8 --mincats 1 --maxcats 4 --prior simplicity --weight 1.0 --noise 0.01 --bottleneck 2 --exposures 2 --mcmc_iterations 5000
```

The results will be written out to the file ```my/results/directory/1``` (the number ```1``` in the above command represents the chain number and can be varied to write each chain to a different file). Depending on the parameter settings you choose, running a single chain can take several hours. In general, the simplicity prior is slower to compute than the informativeness prior. Running a large number of chains is best performed on a cluster (for example, run one chain per CPU core). Each line in the output file gives the results from a single generation.

### Example: Creating agents that learn from data

If you just want to experiment with the model, it might be more convenient to run the code from an interactive Python shell. This will allow you to do things like this:

```
>>> import model
>>> my_agent = model.Agent(shape=(4,4), prior='simplicity', weight=1.0, noise=0.01, exposures=2)
>>> my_data = [((0,0), 0), ((1,0), 0), ((0,2), 1), ((1,2), 1), ((2,1), 2), ((3,1), 2), ((2,3), 3), ((3,3), 3)]
>>> my_agent.learn(my_data)
>>> my_agent.language
array([[0, 2, 1, 3],
       [0, 2, 1, 3],
       [0, 2, 1, 3],
       [0, 2, 1, 3]])
```

In this example, we first created an agent with various parameters (the shape parameter determines the size of the universe – here a 4×4 universe). We then constructed some data (data is a list of meaning–signal pairs, where each meaning is a point in the universe). We then got the agent to learn from this data. Finally, we took a look at what language the agent inferred. Languages are Numpy arrays; each number represents a category/signal, so in this case, the agent has inferred a language which breaks the space up into four vertical stripes. We could also ask the agent to produce a signal for a particular meaning:

```
>>> my_agent.speak((3,3))
3
```

Asked to produce a signal for meaning ```(3,3)```, the agent produces signal ```3```. Of course, there is a small probability that the agent might make a production error, which is determined by the noise parameter we set above (in this example, 1%). To ask the agent to produce signals for all meanings, use ```speak_all()```:

```
>>> my_agent.speak_all()
array([[0, 0, 1, 3],
       [0, 2, 1, 3],
       [0, 2, 1, 3],
       [0, 2, 1, 3]])
```

In this example, the agent has made a production error; meaning ```(0,1)``` has been mislabeled as ```0``` rather than ```2```.

### Example: Running an iterated learning chain

We can now do some iterated learning. First, we'll construct a data set for a few meanings:

```
>>> new_data = [(meaning, my_agent.speak(meaning)) for meaning in [(0,0), (1,1), (2,2), (3,3)]]
>>> new_data
[((0, 0), 0), ((1, 1), 2), ((2, 2), 1), ((3, 3), 3)]
```

Now we'll create a new agent, who will learn from this rather impoverished dataset:

```
>>> my_agent2 = model.Agent(shape=(4,4), prior='simplicity', weight=1.0, noise=0.01, exposures=2)
>>> my_agent2.learn(new_data)
>>> my_agent2.language
array([[0, 2, 1, 3],
       [0, 2, 1, 3],
       [0, 2, 1, 3],
       [0, 2, 1, 3]])
```

Even though the new agent only saw signals for four of the meanings (the meanings on the diagonal), it still managed to infer exactly the same language hypothesis as the previous agent. This is because it has a simplicity prior: Given a small amount of data, it looks for the simplest explanation for that data, which is the stripy partition.

We could continue in this fashion, creating a new agent who learns from new data generated by the previous agent. Instead, we will use a ```Chain``` object to automatically run an iterated learning chain starting from an initially random language. We set up and run a chain like this:

```
>>> my_chain = model.Chain(generations=10, shape=(4,4), prior='simplicity', weight=1.0, noise=0.01, bottleneck=2, exposures=2)
>>> my_chain.simulate()
```

specifying whatever particular parameters we want to test. Under the parameters above, this will take around 20 seconds to run. The results are stored in the list ```my_chain.generations``` (you can also pass a filename to the ```simulate()``` method in order to have the results written out to a file). Let's have a look at generation 0:

```
>>> my_chain.generations[0]
{'language': array([[0, 2, 1, 2], [1, 3, 0, 1], [1, 0, 3, 3], [2, 0, 3, 2]]), 'productions': array([[0, 2, 1, 2], [1, 3, 0, 1], [1, 0, 3, 3], [2, 0, 3, 2]]), 'data_out': [((0, 1), 2), ((1, 0), 1), ((0, 3), 2), ((1, 3), 1), ((2, 1), 0), ((3, 1), 0), ((2, 3), 3), ((3, 3), 2)], 'filtered_agent': False, 'lang_expressivity': 4, 'prod_expressivity': 4, 'lang_error': None, 'prod_error': None, 'lang_complexity': 96.9399527356992, 'prod_complexity': 96.9399527356992, 'lang_cost': 2.9863784237091693, 'prod_cost': 2.9863784237091693, 'model_parameters': {'shape': (4, 4), 'mincats': 1, 'maxcats': 4, 'prior': 'simplicity', 'weight': 1.0, 'noise': 0.01, 'bottleneck': 2, 'exposures': 2, 'mcmc_iterations': 5000}}
```

These results are for a dummy agent that was used to initialize the chain. In particular, we might be interested in looking at the initial random language and its complexity:

```
>>> my_chain.generations[0]['language']
array([[0, 2, 1, 2],
       [1, 3, 0, 1],
       [1, 0, 3, 3],
       [2, 0, 3, 2]])
>>> my_chain.generations[0]['lang_complexity']
96.9399527356992
```

Quite a complex language, which is to be expected because it's random. Now, lets take a look at generation 10:

```
>>> my_chain.generations[10]['language']
array([[2, 2, 2, 2],
       [2, 2, 3, 3],
       [2, 2, 3, 3],
       [2, 2, 3, 3]])
>>> my_chain.generations[10]['lang_complexity']
20.1357092861044
```

Much simpler! The language has simplified to two contiguous categories and the complexity is now about 20 bits. Finally, lets make a quick and dirty plot showing how complexity varies with generation:

```
>>> import matplotlib.pyplot as plt
>>> complexity_scores = [generation['lang_complexity'] for generation in my_chain.generations]
>>> plt.plot(complexity_scores)
>>> plt.show()
```

For nicer plots, visualizations, and animations, code can be found in ```code/il_visualize.py```, ```code/il_animations.py```, ```code/il_results.py```, and ```code/visualize.py```. However, most of this code is specific to the 8×8 case.

### Reproducing results reported in the paper

Various functions are provided in ```code/model_resuts.py``` to reproduce figures and visualizations for the model. Generally, these functions can produce SVG, PDF, EPS, or PNG files, but formats other than SVG require Inkscpae to be installed on your machine. Producing animated gifs requires the Python package imageio.


Experiments
------------

Various functions are provided in ```code/exp1_results.py``` and ```code/exp2_results.py``` to reproduce figures and visualizations for the experiments. Generally, these functions can produce SVG, PDF, EPS, or PNG files, but formats other than SVG require Inkscpae to be installed on your machine. Producing animated gifs requires the Python package imageio. The model fit figure can be reproduced from ```code/mf_results.py``` and requires scikit-optimize to be installed. The model fit analysis was performed by ```code/mf_collect.py``` and ```code/mf_sample.py``` – reproducing the model fit analysis could be tricky because it's extremely resource intensive (it took about three weeks on an HPC cluster) and the code is somewhat tailored to our specific HPC cluster.

### Reproducing the experimental statistics

All statistics were done in R using the lme4 package. They can be reproduced by running the R scripts ```stats/exp1.R``` and ```stats/exp2.R```. Our canonical output of these scripts can be found in ```stats/exp1.R.out``` and ```stats/exp2.R.out```.

### Running the experiments

All the code for the experiments is in the ```experiments/``` directory. The experiments are written in Node.js. To run the experiments you will need a web server with [Node.js](https://nodejs.org) and [MongoDB](https://www.mongodb.com) installed. On a Mac, this can be accomplished by doing something like:

```
brew install node
brew install mongodb
```

Launch MongoDB by doing:

```
mongod
```

and leave it running in the background or set it up as a system service. Place the contents of this repo in some directory on your server and ```cd``` into that directory. Then install the Node.js dependencies using:

```npm install```

Start the Node.js server using:

```node server.js```

If everything's set up right, you'll get a message like this:

```Listening on port 9000```

N.B. The first time you run the server, a MongoDB collection called ```shepard``` will be created automatically. If this happens to clash with a collection you've already set up for something else, you'll need to modify this in ```server.js```.

```server.js``` is the script that runs on the server-side and does most of the experiment logic (randomization, iteration, participant exclusion, writing to the database, etc.). The script contains several parameters at the top of the code for controlling how the experiment works – these are fairly well documented. ```client.js``` is the script that runs on the client side and mostly deals with rendering of the stimuli, responding to button clicks, and so forth. The port numbers at the top of ```server.js``` and ```client.js``` must match. The code is also able to handle a live communication experiment which we never actually ran (this has not been tested in depth).


License
-------

Except where otherwise noted, this repository is licensed under a Creative Commons Attribution 4.0 license. You are free to share and adapt the material for any purpose, even commercially, as long as you give appropriate credit, provide a link to the license, and indicate if changes were made. See LICENSE.md for full details.